---
published: true
---
# Masking

In natural language processing tasks, it is typical to get all the sequences to be of the same length. We decide on a fixed length for the sequences and pad them to match the length. For sentiment classification tasks, we pad the vector form of the sequences, or the reviews in the dataset, with 0 at the end. And for predicting the next word or character, we pad the vector form of the sequences with 0 at the beginning. We need a way to let our neural network know that some part of the data is actually padding and should be ignored. That mechanism is called masking.

