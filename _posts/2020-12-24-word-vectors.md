---
published: false
---
# Ways to associate a vector with a word

## One-Hot Word Vectors
1.Sparse: mostly made of zeros and thus sparse.

2.High-dimensional: the same dimensionality as the number of words in the vocabulary.

3.Hard-coded

4.One-hot encoding word vectors, usually, have vectors that are 20,000-dimensional, for example, for capturing a vocabulary of 20,000 word tokens.

## Word Embeddings
1.Dense: word embeddings take fewer dimensions and therefore dense as opposed to sparse vectors.

2.Lower-dimensional: low-dimensional floating-point vectors.

3.Learned from the data

4.It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1024-dimensional when dealing with very large vocabularies.

